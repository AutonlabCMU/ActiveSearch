\documentclass[12pt]{article}

\usepackage{amsmath,amssymb,hyperref,listings,graphicx,bbold}

\setlength{\parindent}{0pt}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\inv}[1]{#1^{-1}}

\title{Kernel-based Active Search on Graphs}
\date{\vspace{-10ex}}

\begin{document}
\maketitle

\section{Introducing AS on graphs}

Here is the energy function used for AS:
$$E(f) = \sum\limits_{i \in \L} (y_i-f_i)^2 D_{ii} + \lambda w_0 \sum\limits_{i \in \U} (f_i-\pi)^2 D_{ii} + \sum\limits_{i,j} (f_i-f_j)^2 A_{ii}$$

Here is the energy function rewritten using matrices, where $f_L$ and $f_U$ are the $f$-vector portions belonging to the labeled and unlabeled portions respectively (they have been rearranged WLOG):

$$E(f) = \left[\begin{matrix} f_L \\ f_U \\ y \\ \pi \end{matrix}\right]^T \
		 \left[\begin{array}{c|c}
		 	\left[\begin{matrix} D_L & 0 \\ 0&\lambda w_0 D_U\end{matrix}\right] + \lambda(D-A) & 
		 	\left[\begin{matrix} -D_L & 0 \\ 0&-\lambda w_0 D_U\end{matrix}\right] 
		 	\\\\ \hline \\ 
		 	\left[\begin{matrix} -D_L & 0 \\ 0&-\lambda w_0 D_U\end{matrix}\right] & 
		 	0
		 	\end{array}\right] 
		 \left[\begin{matrix} f_L \\ f_U \\ y \\ \pi \end{matrix}\right]
$$

The minimizer is as follows (not proven here): $$f^* = \inv{(I-A')} D' y'$$
where 
$$A' = \left[\begin{matrix} \frac{\lambda}{1+\lambda}I_L & 0 \\ 0 & \frac{1}{1+w_0}I_U \end{matrix}\right] \inv{D} A,\hspace{5mm}
  D' = \left[\begin{matrix} \frac{1}{1+\lambda}I_L & 0 \\ 0 & \frac{w_0}{1+w_0}I_U \end{matrix}\right],\hspace{5mm}
  y' = \left[\begin{matrix} y_L \\ \pi \end{matrix} \right]
$$

If we set $B = \left[\begin{matrix} \frac{\lambda}{1+\lambda}I_L & 0 \\ 0 & \frac{1}{1+w_0}I_U \end{matrix}\right]$, we have that $A' = B\inv D A, \hspace{5mm} D' = I-B$

Thus, we have our optimal solution: $$f^* = \inv{(I - B\inv D A)}(I-B)y'$$

\section{Kernel AS -- Linear Kernel as similarity}

Say $A = X^T X$ where $X = [F(x_1) \hdots F(x_n)]$, with $n$ data points and $r$ features.

Then $D = diag (X^T X \mathbb{1})$. (Precomputed in $O(nr)$).\\

Thus, $$f^* = \inv{(I - \overline{B} X^TX)}q$$

where $\overline{B} = B \inv{D}, q = (I-B)y'$.

Here, we use the Kailath variant of the matrix inverse lemma:
$$\inv{(A+BC)} = \inv{A} -  \inv{A}B\inv{(I+C\inv{A}B)}C\inv{A}$$

We have: 
\begin{tabbing}
$f^* $ \= $= \inv{(I - \overline{B} X^TX)}q$\\
\> $=(I+(\overline{B}X^T)\inv{(I-X\overline{B}X^T)}X)q$\\
\>$=q + \overline{B}X^T\inv{(I-X\overline{B}X^T)}Xq$
\end{tabbing}

The inverse can precomputed in $O(r^2n + r^3)$. 
So the entire precomputation is in $O(r^2n)$ assuming $n > r$.

We want to compute the updates in $O(r^2 + nr)$.

\subsection{Updates to $f$}

We have precomputed $\inv{(I-X\overline{B}X^T)}$. One element in $\overline{B}$ changes.

$$\overline{B}' = \overline{B} -\gamma e_i e_i^T$$
where $e_i$ is the $i^{th}$ standard basis vector.

Let $K = (I-X\overline{B}X^T)$.

Then,
\begin{tabbing} 
$K'$ \= $:=I - X\overline{B}'X^T$\\
\>$= K + \gamma X e_i e_i^T X^T$\\
\>$= K + \gamma x_i x_i^T$
\end{tabbing}

Here, $\gamma = -(\frac{\lambda}{1+\lambda}-\frac{1}{1+w0})\inv{D_ii}$.

Woodbury's Matrix inversion formula: 
$$\inv{(A+UCV)} = \inv{A} 0 \inv{A}U\inv{(\inv{C}+V\inv{A}U)}V\inv{A}$$

Using this, we have:
\begin{tabbing}
$\inv{K'}$\= $= \inv{K} - \inv{K}(\gamma x_i)\inv{(1+ \gamma x_i^T\inv{K}x_i)}x_i^T\inv{K}$\\
\>$=\inv{K} - \frac{\gamma\inv{K}x_i x_i^T\inv{K}}{1+\gamma x_i^T \inv{K} x_i}$\\
\>$=\inv{K} - \frac{\gamma(\inv{K}x_i) (\inv{K}x_i)^T}{1+\gamma x_i^T \inv{K} x_i}$
\end{tabbing}

Further, one element in $q$ changes. $q'_i = y_i \frac{1}{1+\lambda}$

Thus, with the update to the inverse, $f* = q' + \overline{B}'X^T\inv{K'}Xq'$.
This takes $O(rn)$.

\subsection{Impact factor computation}



\end{document}